{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b28984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import statsmodels\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "import datetime as dt\n",
    "\n",
    "from factor_analyzer import FactorAnalyzer, ConfirmatoryFactorAnalyzer, ModelSpecificationParser\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import zscore, chi2, norm, multivariate_normal, spearmanr, pearsonr, shapiro\n",
    "from scipy.optimize import minimize\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8d766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data, drop test entry\n",
    "df = pd.read_csv(\"qualtrics_export.csv\", on_bad_lines=\"skip\", delimiter=\";\")\n",
    "df = df[df[\"Finished\"] == \"TRUE\"].drop(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302a2df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average time to complete survey\n",
    "times = df.iloc[2:][[\"StartDate\", \"EndDate\"]]\n",
    "times[\"total\"] = pd.to_datetime(times[\"EndDate\"]) - pd.to_datetime(times[\"StartDate\"])\n",
    "times[\"total\"].sort_values().iloc[:-1].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba94619-42e1-4e06-b07a-824512c21106",
   "metadata": {},
   "source": [
    "## Drop evaluations that were made too quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dee5060",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_times = []\n",
    "qs = []\n",
    "\n",
    "for row in df.T.iloc[17:].itertuples():\n",
    "    if row[0].endswith(\"Page Submit\"):\n",
    "        \n",
    "        # Questions 201-361 are the actual explanation pair questions\n",
    "        qnum = int(row[0].split(\"_\")[0][1:])\n",
    "        \n",
    "        if 201 <= qnum <= 362:\n",
    "            times = np.array([float(i) for i in row[1:]])\n",
    "            times_filtered = [i for i in times if not np.isnan(i)]\n",
    "            all_times.extend(times_filtered)\n",
    "            qs.extend([qnum] * len(times_filtered))\n",
    "\n",
    "all_times = np.array(all_times)\n",
    "qs = np.array(qs)\n",
    "b10 = np.quantile(all_times, q=.1)\n",
    "print(f\"Bottom 10th percetile of time per question: {b10}s\")\n",
    "print(f\"Number of evaluations that took longer than b10: {len(qs[all_times >= b10])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aaec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(qs[all_times >= b10], return_counts=True)\n",
    "\n",
    "plt.figure(figsize=(12, 6))  # Increase figure size\n",
    "plt.bar(unique, counts, color='skyblue', edgecolor='black')  # Add color and edge for clarity\n",
    "plt.xlabel('Question Number', fontsize=12)\n",
    "plt.ylabel('Number of Usable Answers', fontsize=12)\n",
    "plt.title('Usable Answers Per Question - 10th percentile', fontsize=14)\n",
    "plt.xticks(unique[::10], rotation=45, fontsize=10)  # Show every 10th tick, rotate for readability\n",
    "plt.tight_layout()  # Adjust layout to avoid clipping\n",
    "\n",
    "plt.axhline(y=10, color='red', linestyle='-', linewidth=1, label='Threshold (y=10)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904f00cf",
   "metadata": {},
   "source": [
    "# General descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7dc33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Education\n",
    "df[\"Q3\"].iloc[2:].value_counts().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edc73c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender\n",
    "df[\"Q1\"].iloc[2:].value_counts().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d69564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age\n",
    "df[\"Q2\"].iloc[2:].dropna().apply(int).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93176d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Job type\n",
    "df[\"Q4\"].iloc[2:].str.lower().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14502fb",
   "metadata": {},
   "source": [
    "# Converting Likert to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aba012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just the cognitive orientation questions per participant\n",
    "df_co = df[[\"PROLIFIC_PID\", \"Q1.1\", \"Q2 \", \"Q3.1\", \"Q4.1\", \"Q5\", \"Q6\",\n",
    "            \"Q7\", \"Q8\", \"Q9\", \"Q10\", \"Q11\", \"Q12\", \n",
    "            \"Q13\", \"Q14\", \"Q15\", \"Q16\", \"Q17\", \"Q18\", \n",
    "            \"Q19\", \"Q28\", \"Q29\", \"Q30\", \"Q31\", \"Q32\", \n",
    "            \"Q33\"]].T\n",
    "\n",
    "# Drop columns by testers\n",
    "df_co = df_co.rename(columns = {0 : \"Question text\"})\n",
    "\n",
    "# Set column names to prolific IDs\n",
    "new_headers = df_co.iloc[0, 1:].values  \n",
    "first_column_header = \"Question text\"\n",
    "df_co.columns = [first_column_header] + new_headers.tolist()\n",
    "df_co = df_co.drop(index = \"PROLIFIC_PID\")\n",
    "\n",
    "# Rescale\n",
    "df_co = df_co.replace({\"Strongly disagree\" : 0, \"Somewhat disagree\" : 1,\n",
    "                       \"Neither agree nor disagree\" : 2, \"Somewhat agree\" : 3,\n",
    "                       \"Strongly agree\" : 4})\n",
    "\n",
    "df_co.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e781454f",
   "metadata": {},
   "outputs": [],
   "source": [
    "co_groups = {\"Need for Cognition\" : {\n",
    "               \"Q1.1\" : {\"Reverse\" : True},\n",
    "               \"Q2 \"  : {\"Reverse\" : False},\n",
    "               \"Q3.1\" : {\"Reverse\" : True},\n",
    "               \"Q4.1\" : {\"Reverse\" : True},\n",
    "               \"Q5\"   : {\"Reverse\" : False}\n",
    "               },\n",
    "            \"Need for closure\" : {\n",
    "               \"Q6\"  : {\"Reverse\" : False},\n",
    "               \"Q7\"  : {\"Reverse\" : True},\n",
    "               \"Q8\"  : {\"Reverse\" : True},\n",
    "               \"Q9\"  : {\"Reverse\" : False}, \n",
    "               \"Q10\" : {\"Reverse\" : False},\n",
    "               \"Q11\" : {\"Reverse\" : False},\n",
    "               \"Q12\" : {\"Reverse\" : False}\n",
    "            },\n",
    "            \"Susceptibility to persuasion\" : {\n",
    "                \"Q13\" : {\"Reverse\" : False},\n",
    "                \"Q14\" : {\"Reverse\" : False},\n",
    "                \"Q15\" : {\"Reverse\" : False},\n",
    "                \"Q16\" : {\"Reverse\" : False}\n",
    "            },\n",
    "            \"Skepticism\" : {\n",
    "                \"Q17\" : {\"Reverse\" : False},\n",
    "                \"Q18\" : {\"Reverse\" : True},\n",
    "                \"Q19\" : {\"Reverse\" : False},\n",
    "                \"Q28\" : {\"Reverse\" : False},\n",
    "                \"Q29\" : {\"Reverse\" : False}\n",
    "            },\n",
    "            \"AI Expertise\" : {\n",
    "                \"Q30\" : {\"Reverse\" : False},\n",
    "                \"Q31\" : {\"Reverse\" : False},\n",
    "                \"Q32\" : {\"Reverse\" : False},\n",
    "                \"Q33\" : {\"Reverse\" : True}\n",
    "            }\n",
    "        }\n",
    "\n",
    "sub_dfs = {}\n",
    "\n",
    "for group, questions in co_groups.items():\n",
    "    temp = df_co.loc[list(questions.keys())]\n",
    "    \n",
    "    for question, reverse in questions.items():\n",
    "        if reverse[\"Reverse\"]:    \n",
    "            temp.loc[question] = temp.loc[question].replace({4:0, 3:1, 1:3, 0:4})\n",
    "            \n",
    "    temp = temp.drop(columns = \"Question text\")\n",
    "    \n",
    "    sub_dfs[group] = temp\n",
    "    \n",
    "sub_dfs[\"Need for Cognition\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3f5ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scaled = pd.concat(list(sub_dfs.values()))\n",
    "\n",
    "# Drop incomplete responses\n",
    "all_scaled = all_scaled.drop(columns=all_scaled.columns[all_scaled.isna().sum() > 0]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f91327",
   "metadata": {},
   "source": [
    "# Confirmatory Factor Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9619db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The questions per factor\n",
    "model_dict = {k : list(v.keys()) for k, v in co_groups.items()}\n",
    "\n",
    "# Run CFA\n",
    "model_spec = ModelSpecificationParser.parse_model_specification_from_dict(all_scaled, model_dict)\n",
    "cfa = ConfirmatoryFactorAnalyzer(model_spec, disp=False) \n",
    "cfa.fit(all_scaled.values) \n",
    "cfa.loadings_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dcc877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop scores below the threshold (.3)\n",
    "scores = cfa.loadings_\n",
    "scores = np.array(scores).flatten()\n",
    "scores = scores[scores != 0]\n",
    "\n",
    "co_groups_updated = defaultdict(dict)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "to_drop = []\n",
    "\n",
    "for group, questions in co_groups.items():\n",
    "    for question in questions:\n",
    "        # Q33 is 'objective' and should therefore be included regardless of self-report alignment\n",
    "        if scores[counter] > .3 or question == \"Q33\":\n",
    "            \n",
    "            co_groups_updated[group][question] = co_groups[group][question]\n",
    "        \n",
    "        else:\n",
    "            print(f\"Dropping: {question}\")\n",
    "            to_drop.append(question)\n",
    "\n",
    "        counter += 1\n",
    "        \n",
    "co_groups = co_groups_updated\n",
    "all_scaled_corrected = all_scaled.drop(columns=to_drop)\n",
    "\n",
    "### Rerun CFA with updated questions list\n",
    "# The questions per factor\n",
    "model_dict = {k : list(v.keys()) for k, v in co_groups_updated.items()}\n",
    "\n",
    "# Run CFA\n",
    "model_spec = ModelSpecificationParser.parse_model_specification_from_dict(all_scaled_corrected,\n",
    "                                                                          model_dict)\n",
    "cfa = ConfirmatoryFactorAnalyzer(model_spec, disp=False) \n",
    "cfa.fit(all_scaled_corrected.values) \n",
    "cfa.loadings_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325ac8bf",
   "metadata": {},
   "source": [
    "# Calculating concept scores per participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c094db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average scores for each concept\n",
    "concept_scores = pd.DataFrame()\n",
    "\n",
    "for concept, questions in co_groups.items():\n",
    "    concept_scores[concept] = all_scaled[list(questions.keys())].sum(axis=1) / (len(questions.keys()) * 4)\n",
    "\n",
    "# Output results\n",
    "concept_scores.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7569a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in concept_scores.columns:\n",
    "    print(f\"{col}: {shapiro(concept_scores[col])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa834c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the distributions\n",
    "def plot_concept_score_distributions(dataframe):\n",
    "    num_columns = len(dataframe.columns)\n",
    "    num_rows = (num_columns + 2) // 3  # Arrange in rows of 3 plots\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, 3, figsize=(15, num_rows * 5))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, column in enumerate(dataframe.columns):\n",
    "        sns.histplot(dataframe[column], kde=True, ax=axes[i], bins=15)\n",
    "        axes[i].set_title(column)\n",
    "        axes[i].set_xlabel(\"Score\")\n",
    "        axes[i].set_xlim(0, 1)\n",
    "        axes[i].set_ylabel(\"Frequency\")\n",
    "\n",
    "    # Turn off unused axes\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "plot_concept_score_distributions(concept_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8111d40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_scores.to_csv(\"../results/cognitive_orientations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafb5451",
   "metadata": {},
   "source": [
    "# Data restructuring (evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cc4613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for columns related to explanation pair evaluations and participant-level identifiers\n",
    "pair_columns = [col for col in df.columns if (\"v\" in str(col)) or (\"Page Submit\" in str(col))]\n",
    "participant_columns = ['ResponseId', 'PROLIFIC_PID']\n",
    "subset_data = df[participant_columns + pair_columns]\n",
    "\n",
    "# Find submit times of all questions\n",
    "submits = sorted([i for i in pair_columns if re.match(\"Q\\\\d{3}_Page Submit\", i)])[:-4]\n",
    "qs = sorted(list(set([i[:-2] for i in pair_columns if re.match(\"\\\\d{6}v\\\\d{6}_\\\\d\", i)])))\n",
    "\n",
    "lookup = dict(zip(submits, qs))\n",
    "\n",
    "# Actually remove any evaluations that took less than b10 from the dataset\n",
    "for row in subset_data.iterrows():\n",
    "\n",
    "    index, row = row\n",
    "    \n",
    "    for submit in submits:\n",
    "        if float(row[submit]) < b10:\n",
    "            subset_data.loc[index, f\"{lookup[submit]}_1\"] = np.nan\n",
    "            subset_data.loc[index, f\"{lookup[submit]}_2\"] = np.nan\n",
    "            subset_data.loc[index, f\"{lookup[submit]}_3\"] = np.nan\n",
    "\n",
    "subset_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff6c8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the explanation pair data into long format\n",
    "\n",
    "# Melt explanation pair columns to create a long-format dataframe\n",
    "\n",
    "long_data = pd.melt(\n",
    "    subset_data,\n",
    "    id_vars=participant_columns,  # Keep participant identifiers\n",
    "    value_vars=pair_columns,  # Explanation pair columns\n",
    "    var_name=\"ExplanationPair_Metric\",  # New column for explanation pair + metric\n",
    "    value_name=\"Evaluation\"  # Column for evaluation (trust, transparency, usefulness)\n",
    ")\n",
    "\n",
    "# Split ExplanationPair_Metric into ExplanationPair and MetricType\n",
    "long_data[['ExplanationPair', 'MetricType']] = long_data['ExplanationPair_Metric'].str.extract(r'(.*)_([1-3])')\n",
    "\n",
    "# Map MetricType to human-readable labels\n",
    "metric_map = {\n",
    "    \"1\": \"Trustworthiness\",\n",
    "    \"2\": \"Transparency\",\n",
    "    \"3\": \"Usefulness\"\n",
    "}\n",
    "\n",
    "long_data['MetricType'] = long_data['MetricType'].map(metric_map)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "long_data = long_data.drop(columns = [\"ExplanationPair_Metric\"])\n",
    "long_data = long_data.dropna(subset = [\"Evaluation\", \"ExplanationPair\"])\n",
    "\n",
    "# Display a sample of the reshaped data\n",
    "long_data.sort_values(by=[\"PROLIFIC_PID\", \"ExplanationPair\"]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b493aa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_values(pair):\n",
    "    \"\"\"\n",
    "    The bits used to represent the explanations are counterintuitive for\n",
    "    Persuasion, Detail, and Formality. Hence, we flip them. As a result, the coding is:\n",
    "    bit 0: 0 = low-stakes,       1 = high-stakes\n",
    "    bit 1: 0 = short,            1 = long\n",
    "    bit 2: 0 = running,          1 = bulleted\n",
    "    bit 3: 0 = informal,         1 = formal\n",
    "    bit 4: 0 = aggregated,       1 = comprehensive\n",
    "    bit 5: 0 = decision-support, 1 = persuasive\n",
    "    \"\"\"\n",
    "    \n",
    "    pair = list(pair)\n",
    "    pair[3], pair[10] = \"0\" if int(pair[3]) else \"1\", \"0\" if int(pair[10]) else \"1\"\n",
    "    pair[4], pair[11] = \"0\" if int(pair[4]) else \"1\", \"0\" if int(pair[11]) else \"1\"\n",
    "    pair[5], pair[12] = \"0\" if int(pair[5]) else \"1\", \"0\" if int(pair[12]) else \"1\"\n",
    " \n",
    "    \n",
    "    return \"\".join(pair)\n",
    "    \n",
    "long_data[\"ExplanationPair\"] = long_data[\"ExplanationPair\"].apply(flip_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357e533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_parser(explanation_pair):\n",
    "    \"\"\"\n",
    "    Converts the string-like representation of the explanation pair\n",
    "    to a categorical variable indicating which of the design dimensions\n",
    "    has been 'flipped' between the two explanations. In this flip, explanation\n",
    "    B always has a 1 ('higher' value) and explanation A a 0 ('lower' value)\n",
    "    \"\"\"\n",
    "    \n",
    "    explanation_a, explanation_b = explanation_pair.split(\"v\")\n",
    "    feature_names = [\"Domain\", \"Length\", \"Structure\", \"Formality\", \"Detail\", \"Persuasiveness\"]\n",
    "    \n",
    "    features_a = {f\"{feature}\": int(bit) for feature, bit in zip(feature_names, explanation_a)}\n",
    "    features_b = {f\"{feature}\": int(bit) for feature, bit in zip(feature_names, explanation_b)}       \n",
    "\n",
    "    return {\"Domain\": \"high\" if features_a[\"Domain\"] else \"low\", \n",
    "            \"flipped_dim\": feature_names[np.argmax([features_a[feature] != features_b[feature] \n",
    "                                         for feature in feature_names])]}\n",
    "        \n",
    "# Apply the function to the ExplanationPair column and expand the result into new columns\n",
    "binary_features = long_data['ExplanationPair'].map(pair_parser).apply(pd.Series)\n",
    "\n",
    "# Concatenate the binary features back into the main dataframe\n",
    "long_data = pd.concat([long_data, binary_features], axis=1)\n",
    "\n",
    "# Drop any unnecessary intermediate columns\n",
    "long_data = long_data.drop(columns=[\"ResponseId\", \"Features_A\", \"Features_B\"],\n",
    "                           errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c8513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include flipped_dim and domain as a column\n",
    "cols = [\"PROLIFIC_PID\", \"ExplanationPair\", \"Need for Cognition\",\n",
    "        \"Need for closure\", \"Susceptibility to persuasion\",\n",
    "        \"Skepticism\", \"AI Expertise\", \"Domain\", \"flipped_dim\", \n",
    "        \"MetricType\", \"Evaluation\"]\n",
    "    \n",
    "# Merge evaluations with cognitive orientations and restructure\n",
    "final_data = long_data.join(concept_scores, \n",
    "                            on=\"PROLIFIC_PID\", \n",
    "                            how=\"left\").sort_values(\n",
    "    by=[\"PROLIFIC_PID\", \"ExplanationPair\"]\n",
    ")[cols]\n",
    "\n",
    "final_data[\"Evaluation\"] = final_data[\"Evaluation\"].replace({\"Explanation A\": 0, \"Explanation B\": 1})\n",
    "\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b6d79e-668f-4d66-bc88-480c5fcdd055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_flipper(row):\n",
    "    \"\"\"\n",
    "    Since we flipped three dimensions, their direction in terms of evaluation\n",
    "    has also been flipped (i.e., explanation A (baseline) has \"become\" explanation B (treatment)\n",
    "    since that one now has the 1 instead of the 0. As a result, we also have to flip the order of the \n",
    "    explanations and the evaluation for it, so that a 1 again indicates a preference for the treatment. \n",
    "    \"\"\"\n",
    "\n",
    "    split = row[\"ExplanationPair\"].split(\"v\")\n",
    "    reversed_order = f\"{split[1]}v{split[0]}\"\n",
    "    \n",
    "    if row[\"flipped_dim\"] in [\"Formality\", \"Detail\", \"Persuasiveness\"]:\n",
    "        return 0 if row[\"Evaluation\"] else 1, reversed_order \n",
    "    else:\n",
    "        return row[\"Evaluation\"], row[\"ExplanationPair\"]\n",
    "\n",
    "# Apply flipper\n",
    "for row in final_data.iterrows():\n",
    "    fixed_eval, fixed_explanation = decision_flipper(row[1])\n",
    "    \n",
    "    final_data.loc[row[0], \"Evaluation\"] = fixed_eval\n",
    "    final_data.loc[row[0], \"ExplanationPair\"] = fixed_explanation\n",
    "\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec07d50",
   "metadata": {},
   "source": [
    "# Calculate polychoric correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6430a968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store correlation dataset as csv\n",
    "correlation_df = final_data[[\"PROLIFIC_PID\", \n",
    "                             \"MetricType\", \n",
    "                             \"Evaluation\",\n",
    "                             \"ExplanationPair\"]].pivot(\n",
    "                     columns=\"MetricType\", \n",
    "                     values=\"Evaluation\",\n",
    "                     index=[\"PROLIFIC_PID\", \"ExplanationPair\"]\n",
    "                 )\n",
    "\n",
    "correlation_df.to_csv(\"../results/correlation_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff42e15c-0f2f-4da5-a405-f785dec017b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(polycor)\n",
    "\n",
    "data <- read.csv(\"correlation_df.csv\")\n",
    "\n",
    "calculate_p <- function(corr) {\n",
    "  rho = corr$rho\n",
    "  SE = sqrt(corr$var)\n",
    "  z <- rho / SE\n",
    "  p = 2 * (1 - pnorm(abs(z)))\n",
    "  return(as.numeric(p))\n",
    "}\n",
    "\n",
    "corr_use_trans = polychor(data$Usefulness, data$Transparency, std.err=TRUE)\n",
    "p_use_trans = calculate_p(corr_use_trans)\n",
    "\n",
    "corr_use_trust = polychor(data$Usefulness, data$Trustworthiness, std.err=TRUE)\n",
    "p_use_trust = calculate_p(corr_use_trust)\n",
    "\n",
    "corr_trust_trans = polychor(data$Trustworthiness, data$Transparency, std.err=TRUE)\n",
    "p_trust_trans = calculate_p(corr_trust_trans)\n",
    "\n",
    "print(corr_use_trans$rho)\n",
    "print(p_use_trans)\n",
    "print(corr_use_trust$rho)\n",
    "print(p_use_trust)\n",
    "print(corr_trust_trans$rho)\n",
    "print(p_trust_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b28cfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = final_data.reset_index().drop(columns=\"index\")\n",
    "final_data.columns = [i.replace(\" \", \"_\") for i in final_data.columns]\n",
    "final_data = final_data.dropna()\n",
    "\n",
    "# Calculate share of evaluations where all metrics aligned\n",
    "decisions = final_data.groupby([\"PROLIFIC_PID\", \"ExplanationPair\"])[\"Evaluation\"].sum().values\n",
    "np.isin(decisions, [0, 3]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4117e4b0-2b23-4a8c-b9ec-03f08c4ed000",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a476c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store dataframe used for logistic regressions\n",
    "final_data.to_csv(\"../results/evaluations.csv\")\n",
    "\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9727891a-68fc-4bf9-9ff5-3b657901b0fa",
   "metadata": {},
   "source": [
    "## Fitting the mixed-effects logistic regression model (without personalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabf6cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Load required libraries\n",
    "library(lme4)\n",
    "library(dplyr)\n",
    "library(stats)\n",
    "\n",
    "# Load the data\n",
    "data <- read.csv(\"../results/evaluations.csv\")\n",
    "\n",
    "# Set categorical variables\n",
    "data$flipped_dim <- factor(data$flipped_dim, \n",
    "                           levels = c(\"Structure\", \"Length\", \"Formality\", \"Detail\", \"Persuasiveness\"))\n",
    "\n",
    "data$MetricType <- factor(data$MetricType, \n",
    "                          levels = c(\"Trustworthiness\", \"Usefulness\", \"Transparency\"))\n",
    "\n",
    "data$Domain <- factor(data$Domain, \n",
    "                      levels = c(\"high\", \"low\"))\n",
    "\n",
    "# Fit the mixed-effects logistic regression model\n",
    "model <- glmer(Evaluation ~ flipped_dim + MetricType + Domain +\n",
    "                 (1 | PROLIFIC_PID), \n",
    "               data = data, \n",
    "               family = binomial,\n",
    "               control = glmerControl(optimizer = \"bobyqa\", \n",
    "                                      optCtrl = list(maxfun = 1e6))\n",
    "               )\n",
    "\n",
    "print(summary(model))\n",
    "\n",
    "# Extract fixed effect coefficients\n",
    "coefs <- fixef(model)\n",
    "\n",
    "# Extract variance-covariance matrix\n",
    "vcov_matrix <- vcov(model)\n",
    "\n",
    "# Separate main effects and interaction terms\n",
    "main_effects <- coefs[grepl(\"^flipped_dim\", names(coefs))]\n",
    "\n",
    "# Derive the missing coefficient for 'Structure' (main effect)\n",
    "structure_main_effect <- -sum(main_effects)\n",
    "\n",
    "# Calculate standard errors for the missing coefficients\n",
    "# Main effect for 'Structure'\n",
    "main_effect_vars <- diag(vcov_matrix)[grepl(\"^flipped_dim\", names(coefs))]\n",
    "main_effect_covs <- vcov_matrix[grepl(\"^flipped_dim\", names(coefs)), grepl(\"^flipped_dim\", names(coefs))]\n",
    "main_effect_se <- sqrt(sum(main_effect_vars) + 2 * sum(main_effect_covs[lower.tri(main_effect_covs)]))\n",
    "\n",
    "# Calculate z-scores and p-values\n",
    "structure_main_z <- structure_main_effect / main_effect_se\n",
    "structure_main_p <- 2 * (1 - pnorm(abs(structure_main_z)))\n",
    "\n",
    "# Combine results into a data frame\n",
    "# Main effects\n",
    "main_results <- data.frame(\n",
    "  Predictor = gsub(\"flipped_dim\", \"\", names(main_effects)),\n",
    "  Coefficient = main_effects,\n",
    "  SE = sqrt(diag(vcov_matrix)[grepl(\"^flipped_dim\", names(coefs))]),\n",
    "  z = main_effects / sqrt(diag(vcov_matrix)[grepl(\"^flipped_dim\", names(coefs))]),\n",
    "  p_value = 2 * (1 - pnorm(abs(main_effects / sqrt(diag(vcov_matrix)[grepl(\"^flipped_dim\", names(coefs))]))))\n",
    ")\n",
    "\n",
    "# Missing coefficients (Structure)\n",
    "structure_results <- data.frame(\n",
    "  Predictor = \"Structure\",\n",
    "  Coefficient = structure_main_effect,\n",
    "  SE = main_effect_se,\n",
    "  z = structure_main_z,\n",
    "  p_value = structure_main_p\n",
    ")\n",
    "\n",
    "# Combine all results, explicitly including the intercept\n",
    "intercept_result <- data.frame(\n",
    "  Predictor = \"(Intercept)\",\n",
    "  Coefficient = coefs[\"(Intercept)\"],\n",
    "  SE = sqrt(diag(vcov_matrix)[\"(Intercept)\"]),\n",
    "  z = coefs[\"(Intercept)\"] / sqrt(diag(vcov_matrix)[\"(Intercept)\"]),\n",
    "  p_value = 2 * (1 - pnorm(abs(coefs[\"(Intercept)\"] / sqrt(diag(vcov_matrix)[\"(Intercept)\"]))))\n",
    ")\n",
    "\n",
    "# Combine all results\n",
    "final_results <- bind_rows(intercept_result, main_results, structure_results)\n",
    "\n",
    "# Format the table and include significance levels\n",
    "final_results <- final_results %>%\n",
    "  mutate(\n",
    "    Coefficient = round(Coefficient, 3),\n",
    "    SE = round(SE, 3),\n",
    "    z = round(z, 3),\n",
    "    p_value = round(p_value, 3),\n",
    "    Significance = case_when(\n",
    "      p_value < 0.001 ~ \"***\",\n",
    "      p_value < 0.01 ~ \"**\",\n",
    "      p_value < 0.05 ~ \"*\",\n",
    "      TRUE ~ \"\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "# Fix ordering\n",
    "rownames(final_results) <- NULL\n",
    "custom_order <- c(\"(Intercept)\", \"Detail\", \"Formality\", \"Length\", \"Persuasiveness\", \"Structure\")\n",
    "\n",
    "final_results <- final_results[match(custom_order, final_results$Predictor), ]\n",
    "\n",
    "write.csv(final_results, \"../results/final_results_general.csv\", row.names = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce079b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Routput = pd.read_csv(\"../results/final_results_general.csv\")\n",
    "\n",
    "corr = statsmodels.stats.multitest.fdrcorrection(Routput[\"p_value\"],\n",
    "                                                 alpha=0.05, \n",
    "                                                 method='indep',\n",
    "                                                 is_sorted=False)\n",
    "\n",
    "Routput[\"Odds Ratio\"] = np.exp(Routput[\"Coefficient\"])\n",
    "Routput[\"Corrected p-value\"] = corr[1]\n",
    "Routput[\"Reject H0\"] = corr[0]\n",
    "\n",
    "Routput[[\"Predictor\", \"Coefficient\", \"SE\", \"Odds Ratio\", \"Corrected p-value\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a3a3d1",
   "metadata": {},
   "source": [
    "## Fitting the mixed-effects logistic regression model (with personalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfee713",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Load required libraries\n",
    "library(lme4)\n",
    "library(dplyr)\n",
    "library(stats)\n",
    "\n",
    "# Load the data\n",
    "data <- read.csv(\"../results/evaluations.csv\")\n",
    "\n",
    "# Set categorical variables\n",
    "data$flipped_dim <- factor(data$flipped_dim, \n",
    "                           levels = c(\"Structure\", \"Length\", \"Formality\", \"Detail\", \"Persuasiveness\"))\n",
    "\n",
    "data$MetricType <- factor(data$MetricType, \n",
    "                          levels = c(\"Trustworthiness\", \"Usefulness\", \"Transparency\"))\n",
    "\n",
    "data$Domain <- factor(data$Domain, \n",
    "                      levels = c(\"high\", \"low\"))\n",
    "\n",
    "# Fit the mixed-effects logistic regression model\n",
    "model <- glmer(Evaluation ~ Need_for_Cognition + Need_for_closure + \n",
    "                 Susceptibility_to_persuasion + Skepticism + \n",
    "                 AI_Expertise + flipped_dim + MetricType + Domain +\n",
    "                 (Need_for_Cognition + Need_for_closure + Susceptibility_to_persuasion + \n",
    "                    Skepticism + AI_Expertise):flipped_dim + \n",
    "                 (1 | PROLIFIC_PID), \n",
    "               data = data, \n",
    "               family = binomial,\n",
    "               control = glmerControl(optimizer = \"bobyqa\", \n",
    "                                      optCtrl = list(maxfun = 1e6))\n",
    "               )\n",
    "\n",
    "print(summary(model))\n",
    "\n",
    "# Extract fixed effect coefficients\n",
    "coefs <- fixef(model)\n",
    "\n",
    "# Extract variance-covariance matrix\n",
    "vcov_matrix <- vcov(model)\n",
    "\n",
    "# Separate main effects and interaction terms\n",
    "main_effects <- coefs[grepl(\"^flipped_dim\", names(coefs))]\n",
    "interaction_terms <- coefs[grepl(\":flipped_dim\", names(coefs))]\n",
    "continuous_effects <- coefs[!grepl(\"flipped_dim\", names(coefs)) & !grepl(\":\", names(coefs))]\n",
    "\n",
    "# Derive the missing coefficient for 'Structure' (main effect)\n",
    "structure_main_effect <- -sum(main_effects)\n",
    "\n",
    "# Derive the missing coefficients for 'Structure' (interactions)\n",
    "predictors <- c(\"Need_for_Cognition\", \"Need_for_closure\", \n",
    "                \"Susceptibility_to_persuasion\", \"Skepticism\", \"AI_Expertise\")\n",
    "structure_interactions <- sapply(predictors, function(pred) {\n",
    "  interaction_coefs <- coefs[grepl(paste0(pred, \":flipped_dim\"), names(coefs))]\n",
    "  -sum(interaction_coefs)\n",
    "})\n",
    "\n",
    "# Calculate standard errors for the missing coefficients\n",
    "# Main effect for 'Structure'\n",
    "main_effect_vars <- diag(vcov_matrix)[grepl(\"^flipped_dim\", names(coefs))]\n",
    "main_effect_covs <- vcov_matrix[grepl(\"^flipped_dim\", names(coefs)), grepl(\"^flipped_dim\", names(coefs))]\n",
    "main_effect_se <- sqrt(sum(main_effect_vars) + 2 * sum(main_effect_covs[lower.tri(main_effect_covs)]))\n",
    "\n",
    "# Interactions for 'Structure'\n",
    "interaction_se <- sapply(1:length(predictors), function(i) {\n",
    "  pred <- predictors[i]\n",
    "  interaction_indices <- which(grepl(paste0(pred, \":flipped_dim\"), names(coefs)))\n",
    "  interaction_vars <- diag(vcov_matrix)[interaction_indices]\n",
    "  interaction_covs <- vcov_matrix[interaction_indices, interaction_indices]\n",
    "  sqrt(sum(interaction_vars) + 2 * sum(interaction_covs[lower.tri(interaction_covs)]))\n",
    "})\n",
    "\n",
    "# Calculate z-scores and p-values\n",
    "structure_main_z <- structure_main_effect / main_effect_se\n",
    "structure_main_p <- 2 * (1 - pnorm(abs(structure_main_z)))\n",
    "\n",
    "interaction_z <- structure_interactions / interaction_se\n",
    "interaction_p <- 2 * (1 - pnorm(abs(interaction_z)))\n",
    "\n",
    "# Combine results into a data frame\n",
    "# Main effects\n",
    "main_results <- data.frame(\n",
    "  Predictor = gsub(\"flipped_dim\", \"\", names(main_effects)),\n",
    "  Coefficient = main_effects,\n",
    "  SE = sqrt(diag(vcov_matrix)[grepl(\"^flipped_dim\", names(coefs))]),\n",
    "  z = main_effects / sqrt(diag(vcov_matrix)[grepl(\"^flipped_dim\", names(coefs))]),\n",
    "  p_value = 2 * (1 - pnorm(abs(main_effects / sqrt(diag(vcov_matrix)[grepl(\"^flipped_dim\", names(coefs))]))))\n",
    ")\n",
    "\n",
    "# Continuous predictors (retain as-is)\n",
    "continuous_results <- data.frame(\n",
    "  Predictor = names(continuous_effects),\n",
    "  Coefficient = continuous_effects,\n",
    "  SE = sqrt(diag(vcov_matrix)[!grepl(\"flipped_dim\", names(coefs)) & !grepl(\":\", names(coefs))]),\n",
    "  z = continuous_effects / sqrt(diag(vcov_matrix)[!grepl(\"flipped_dim\", names(coefs)) & !grepl(\":\", names(coefs))]),\n",
    "  p_value = 2 * (1 - pnorm(abs(continuous_effects / sqrt(diag(vcov_matrix)[!grepl(\"flipped_dim\", names(coefs)) & !grepl(\":\", names(coefs))]))))\n",
    ")\n",
    "\n",
    "# Interaction terms\n",
    "interaction_results <- data.frame(\n",
    "  Predictor = gsub(\":flipped_dim\", \":\", names(interaction_terms)),\n",
    "  Coefficient = interaction_terms,\n",
    "  SE = sqrt(diag(vcov_matrix)[grepl(\":flipped_dim\", names(coefs))]),\n",
    "  z = interaction_terms / sqrt(diag(vcov_matrix)[grepl(\":flipped_dim\", names(coefs))]),\n",
    "  p_value = 2 * (1 - pnorm(abs(interaction_terms / sqrt(diag(vcov_matrix)[grepl(\":flipped_dim\", names(coefs))]))))\n",
    ")\n",
    "\n",
    "# Missing coefficients (Structure)\n",
    "structure_results <- data.frame(\n",
    "  Predictor = c(\"Structure\", paste0(predictors, \":Structure\")),\n",
    "  Coefficient = c(structure_main_effect, structure_interactions),\n",
    "  SE = c(main_effect_se, interaction_se),\n",
    "  z = c(structure_main_z, interaction_z),\n",
    "  p_value = c(structure_main_p, interaction_p)\n",
    ")\n",
    "\n",
    "# Combine all results\n",
    "final_results <- bind_rows(continuous_results, main_results, interaction_results, structure_results)\n",
    "\n",
    "# Format the table and include significance levels\n",
    "final_results <- final_results %>%\n",
    "  mutate(\n",
    "    Coefficient = round(Coefficient, 3),\n",
    "    SE = round(SE, 3),\n",
    "    z = round(z, 3),\n",
    "    p_value = round(p_value, 3),\n",
    "    Significance = case_when(\n",
    "      p_value < 0.001 ~ \"***\",\n",
    "      p_value < 0.01 ~ \"**\",\n",
    "      p_value < 0.05 ~ \"*\",\n",
    "      TRUE ~ \"\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "# Fix ordering\n",
    "rownames(final_results) <- NULL\n",
    "custom_order <- c(\"(Intercept)\", \"Need_for_Cognition\", \"Need_for_closure\", \"Susceptibility_to_persuasion\",\n",
    "                  \"Skepticism\", \"AI_Expertise\", \"Detail\", \"Formality\", \"Length\", \"Persuasiveness\", \"Structure\",\n",
    "                  \"Need_for_Cognition:Detail\", \"Need_for_Cognition:Formality\", \"Need_for_Cognition:Length\",\n",
    "                  \"Need_for_Cognition:Persuasiveness\", \"Need_for_Cognition:Structure\", \"Need_for_closure:Detail\", \n",
    "                  \"Need_for_closure:Formality\", \"Need_for_closure:Length\", \"Need_for_closure:Persuasiveness\", \n",
    "                  \"Need_for_closure:Structure\", \"Susceptibility_to_persuasion:Detail\", \"Susceptibility_to_persuasion:Formality\",\n",
    "                  \"Susceptibility_to_persuasion:Length\", \"Susceptibility_to_persuasion:Persuasiveness\",\n",
    "                  \"Susceptibility_to_persuasion:Structure\", \"Skepticism:Detail\", \"Skepticism:Formality\", \n",
    "                  \"Skepticism:Length\", \"Skepticism:Persuasiveness\", \"Skepticism:Structure\", \"AI_Expertise:Detail\",\n",
    "                  \"AI_Expertise:Formality\",\"AI_Expertise:Length\",\"AI_Expertise:Persuasiveness\",\"AI_Expertise:Structure\")\n",
    "\n",
    "final_results <- final_results[match(custom_order, final_results$Predictor), ]\n",
    "\n",
    "write.csv(final_results, \"../results/final_results_personal.csv\", row.names = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c8492a-90d1-4abd-bd1e-63c1d5af8d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "Routput = pd.read_csv(\"../results/final_results_personal.csv\")\n",
    "\n",
    "corr = statsmodels.stats.multitest.fdrcorrection(Routput[\"p_value\"],\n",
    "                                                 alpha=0.05, \n",
    "                                                 method='indep',\n",
    "                                                 is_sorted=False)\n",
    "\n",
    "Routput[\"Odds Ratio\"] = np.exp(Routput[\"Coefficient\"])\n",
    "Routput[\"Corrected p-value\"] = corr[1]\n",
    "Routput[\"Reject H0\"] = corr[0]\n",
    "\n",
    "Routput[[\"Predictor\", \"Coefficient\", \"SE\", \"Odds Ratio\", \"z\", \"Corrected p-value\", \"Reject H0\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882dc48d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
